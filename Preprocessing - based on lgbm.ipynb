{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/competitions/home-credit-default-risk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buro_bal = pd.read_csv( path +'bureau_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_input():\n",
    "    buro_bal = pd.read_csv(path + 'bureau_balance.csv')\n",
    "    print('Buro bal shape : ', buro_bal.shape)\n",
    "    \n",
    "    print('transform to dummies')\n",
    "    buro_bal = pd.concat([buro_bal, pd.get_dummies(buro_bal.STATUS, prefix='buro_bal_status')], axis=1).drop('STATUS', axis=1)\n",
    "    \n",
    "    print('Counting buros')\n",
    "    buro_counts = buro_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n",
    "    buro_bal['buro_count'] = buro_bal['SK_ID_BUREAU'].map(buro_counts['MONTHS_BALANCE'])\n",
    "    \n",
    "    print('averaging buro bal')\n",
    "    avg_buro_bal = buro_bal.groupby('SK_ID_BUREAU').mean()\n",
    "    \n",
    "    avg_buro_bal.columns = ['avg_buro_' + f_ for f_ in avg_buro_bal.columns]\n",
    "    del buro_bal\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Read Bureau')\n",
    "    buro = pd.read_csv(path + 'bureau.csv')\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    buro_credit_active_dum = pd.get_dummies(buro.CREDIT_ACTIVE, prefix='ca_')\n",
    "    buro_credit_currency_dum = pd.get_dummies(buro.CREDIT_CURRENCY, prefix='cu_')\n",
    "    buro_credit_type_dum = pd.get_dummies(buro.CREDIT_TYPE, prefix='ty_')\n",
    "    \n",
    "    buro_full = pd.concat([buro, buro_credit_active_dum, buro_credit_currency_dum, buro_credit_type_dum], axis=1)\n",
    "    # buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]\n",
    "    \n",
    "    del buro_credit_active_dum, buro_credit_currency_dum, buro_credit_type_dum\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Merge with buro avg')\n",
    "    buro_full = buro_full.merge(right=avg_buro_bal.reset_index(), how='left', on='SK_ID_BUREAU', suffixes=('', '_bur_bal'))\n",
    "    \n",
    "    print('Counting buro per SK_ID_CURR')\n",
    "    nb_bureau_per_curr = buro_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n",
    "    buro_full['SK_ID_BUREAU'] = buro_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n",
    "    \n",
    "    print('Averaging bureau')\n",
    "    avg_buro = buro_full.groupby('SK_ID_CURR').mean()\n",
    "    print(avg_buro.head())\n",
    "    \n",
    "    del buro, buro_full\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Read prev')\n",
    "    prev = pd.read_csv(path +'previous_application.csv')\n",
    "    \n",
    "    prev_cat_features = [\n",
    "        f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "    ]\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    prev_dum = pd.DataFrame()\n",
    "    for f_ in prev_cat_features:\n",
    "        prev_dum = pd.concat([prev_dum, pd.get_dummies(prev[f_], prefix=f_).astype(np.uint8)], axis=1)\n",
    "    \n",
    "    prev = pd.concat([prev, prev_dum], axis=1)\n",
    "    \n",
    "    del prev_dum\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Counting number of Prevs')\n",
    "    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n",
    "    \n",
    "    print('Averaging prev')\n",
    "    avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "    print(avg_prev.head())\n",
    "    del prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading POS_CASH')\n",
    "    pos = pd.read_csv(path +'POS_CASH_balance.csv')\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    pos = pd.concat([pos, pd.get_dummies(pos['NAME_CONTRACT_STATUS'])], axis=1)\n",
    "    \n",
    "    print('Compute nb of prevs per curr')\n",
    "    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    print('Go to averages')\n",
    "    avg_pos = pos.groupby('SK_ID_CURR').mean()\n",
    "    \n",
    "    del pos, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading CC balance')\n",
    "    cc_bal = pd.read_csv(path +'credit_card_balance.csv')\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    cc_bal = pd.concat([cc_bal, pd.get_dummies(cc_bal['NAME_CONTRACT_STATUS'], prefix='cc_bal_status_')], axis=1)\n",
    "    \n",
    "    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    print('Compute average')\n",
    "    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n",
    "    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n",
    "    \n",
    "    del cc_bal, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading Installments')\n",
    "    inst = pd.read_csv(path + 'installments_payments.csv')\n",
    "    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    avg_inst = inst.groupby('SK_ID_CURR').mean()\n",
    "    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n",
    "    \n",
    "    print('Read data and test')\n",
    "    data = pd.read_csv(path +'application_train.csv')\n",
    "    test = pd.read_csv(path +'application_test.csv')\n",
    "    print('Shapes : ', data.shape, test.shape)\n",
    "    \n",
    "    y = data['TARGET']\n",
    "    del data['TARGET']\n",
    "    \n",
    "    categorical_feats = [\n",
    "        f for f in data.columns if data[f].dtype == 'object'\n",
    "    ]\n",
    "    categorical_feats\n",
    "    for f_ in categorical_feats:\n",
    "        data[f_], indexer = pd.factorize(data[f_])\n",
    "        test[f_] = indexer.get_indexer(test[f_])\n",
    "        \n",
    "    data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del avg_buro, avg_prev\n",
    "    gc.collect()\n",
    "\n",
    "    return data, test, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "toc = time.time()\n",
    "train, test, y_train = build_model_input()\n",
    "\n",
    "print(\"Processing time\", time.time() - toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filled = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled = train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled.to_csv('train_filled.csv')\n",
    "test_filled.to_csv('test_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN , SMOTETomek, smote_enn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sme = SMOTEENN(random_state=42)\n",
    "train_res, y_train_res = sme.fit_sample(train_filled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "train_res_under, y_train_under, idx_resampled = rus.fit_sample(train_filled , y_train )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
